{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "my_virtualenv",
      "language": "python",
      "name": "my_virtualenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n13EdKiGadcd"
      },
      "source": [
        "# **Text mining problem description**\r\n",
        "\r\n",
        "For my project, I am attempting to train a spacy NER model to recognize miRNA as a named entity. Although finding miRNA within a document is something that could possibly be done with regex matching, spacy offers many features in the NLP pipeline regarding named entities, and in order to utilize these features with a named entity, it must be able to be recognized by spacy, which cannot be done with regex matching (example below). \r\n",
        "\r\n",
        "![](https://nlpforhackers.io/wp-content/uploads/2018/03/Screen-Shot-2018-03-28-at-12.09.32.png)\r\n",
        "\r\n",
        "Ideally, the miRNA entity would be added to an existing model, such as `scispacy`. By updating `scispacy`'s `en_ner_bionlp13cg_md` model which has a NER for things like `GENE_OR_GENE_PRODUCT`, `ORGANISM`, `PATHOLOGICAL_FORMATION`, and more, we could theoretically obtain the relationship between miRNA and these entities, which could be used on a larger scale to potentially identify genes within literature that may be a ssociated with a given miR to uncover regulatory networks and other associations. \r\n",
        "\r\n",
        "For the purpose of this assignment, we will not be updating an existing model (due to time constraints), but we will run through the process of trianing a model, and at the end identify other potential improvements for this pipeline that would ultimately provide high level, practical performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ja19wnmWdvVp"
      },
      "source": [
        "# 2) **Dataset Description**\r\n",
        "\r\n",
        "To train a NER model in spacy, we need to know the start and end index of the string of interest. Seeing as there is no annotated data in the wild where this exists for miRNA, we will have to make some. We will do this by:\r\n",
        "\r\n",
        "- 1) Fetching miR abstracts from pubmed\r\n",
        "- 2) Cleaning the abstracts and sentence tokenizing\r\n",
        "- 3) Using Regex to find miRNA and their position in the string\r\n",
        "- 4) Format the data to be used by spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1DZbKpko0Jg"
      },
      "source": [
        "### 2.1) Install Dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYSqCW9zpER_"
      },
      "source": [
        "Note, for this exercise, we need to used `spacy 2.2.2`. We will ensure that this is the correct version imported below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ja62mICza1OZ",
        "outputId": "5118fbf8-c188-4a48-982d-28eae05c537f"
      },
      "source": [
        "!pip install xlrd\n",
        "!pip uninstall spacy\n",
        "!pip install spacy==2.2.2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.7/dist-packages (1.1.0)\n",
            "Uninstalling spacy-2.2.2:\n",
            "  Would remove:\n",
            "    /usr/local/bin/spacy\n",
            "    /usr/local/lib/python3.7/dist-packages/bin/*\n",
            "    /usr/local/lib/python3.7/dist-packages/spacy-2.2.2.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/spacy/*\n",
            "  Would not remove (might be manually added):\n",
            "    /usr/local/lib/python3.7/dist-packages/bin/theano_cache.py\n",
            "    /usr/local/lib/python3.7/dist-packages/bin/theano_nose.py\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled spacy-2.2.2\n",
            "Collecting spacy==2.2.2\n",
            "  Using cached https://files.pythonhosted.org/packages/b9/01/fcb8ae3e836fea5c11fdb4c074d27b52bdf74b47bd9bb28a811b7ab37d49/spacy-2.2.2-cp37-cp37m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.2) (2.23.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.2) (1.0.5)\n",
            "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.2) (7.3.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.2) (2.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.2) (54.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.2) (3.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.2) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.2) (0.8.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.2) (1.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.2) (3.7.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.2) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.2) (1.19.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.2) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.2) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.2) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.2) (2.10)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from thinc<7.4.0,>=7.3.0->spacy==2.2.2) (4.41.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy==2.2.2) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy==2.2.2) (3.4.1)\n",
            "Installing collected packages: spacy\n",
            "Successfully installed spacy-2.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CibQZ-zba1Oc"
      },
      "source": [
        "import urllib.request\n",
        "import io\n",
        "import gzip\n",
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "try:\n",
        "    from Bio import Entrez\n",
        "except ModuleNotFoundError:\n",
        "    !pip install Bio\n",
        "    from Bio import Entrez\n",
        "import re\n",
        "import random\n",
        "import spacy"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vcyxIt2pTo3"
      },
      "source": [
        "### 2.2) Creating a database of known miRs and accessions numbers to query literature\r\n",
        "\r\n",
        "To standardize things, we will make a key between miR names and accessions numbers. We will submit accession numbers into python, which will be handlesd by this database to get the corresponding miR, which will be used to query pubmed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qfxo6CGfpR75",
        "outputId": "e2c0aa63-ab23-441d-f502-fe0ac3c904dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "response = urllib.request.urlopen('ftp://mirbase.org/pub/mirbase/CURRENT/miRNA.xls.gz')\r\n",
        "compressed_file = io.BytesIO(response.read())\r\n",
        "decompressed_file = gzip.GzipFile(fileobj=compressed_file)\r\n",
        "\r\n",
        "with open(Path(os.getcwd(), 'miRNA.xlsx'), 'wb') as outfile:\r\n",
        "    outfile.write(decompressed_file.read())\r\n",
        "\r\n",
        "mir_database = pd.read_excel('miRNA.xlsx')\r\n",
        "\r\n",
        "mir_database_1 = mir_database.loc[:, ['Accession', 'ID']]\r\n",
        "mir_database_2 = mir_database.loc[:, ['Mature1_Acc', 'Mature1_ID']].rename(columns = {'Mature1_Acc':'Accession', 'Mature1_ID':'ID'})\r\n",
        "mir_database_3 = mir_database.loc[:, ['Mature2_Acc', 'Mature2_ID']].rename(columns = {'Mature2_Acc':'Accession', 'Mature2_ID':'ID'})\r\n",
        "\r\n",
        "final_database = pd.concat([mir_database_1, mir_database_2, mir_database_3])\r\n",
        "\r\n",
        "final_database.head()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Accession</th>\n",
              "      <th>ID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>MI0000001</td>\n",
              "      <td>cel-let-7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>MI0000002</td>\n",
              "      <td>cel-lin-4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>MI0000003</td>\n",
              "      <td>cel-mir-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>MI0000004</td>\n",
              "      <td>cel-mir-2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>MI0000005</td>\n",
              "      <td>cel-mir-34</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Accession          ID\n",
              "0  MI0000001   cel-let-7\n",
              "1  MI0000002   cel-lin-4\n",
              "2  MI0000003   cel-mir-1\n",
              "3  MI0000004   cel-mir-2\n",
              "4  MI0000005  cel-mir-34"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdmogT4bp0ap"
      },
      "source": [
        "### 2.3) Define functions to 1) fetch abstracts from pubmed, and 2) clean the abstracts and standardize their format, and finally 3) query pubmed and clean the abstract using functions 1 & 2. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtyYyq0_a1Oe"
      },
      "source": [
        "# Function 1\n",
        "\n",
        "def fetch_abstract(pmid):\n",
        "    handle = Entrez.efetch(db='pubmed', id = pmid, retmode='xml')\n",
        "    article = Entrez.read(handle)['PubmedArticle'][0]['MedlineCitation']['Article']\n",
        "    if 'Abstract' in article:\n",
        "            return article['Abstract']['AbstractText']\n",
        "        \n",
        "# Function 2\n",
        "\n",
        "def concat_article(x):\n",
        "    final_article = str()\n",
        "    for i in range(len(x)):\n",
        "        final_article = final_article + str(x[i]) + ' '\n",
        "    return final_article\n",
        "\n",
        "# Function 3\n",
        "\n",
        "def get_literature(user_mir):\n",
        "    \n",
        "    filtered_database = final_database[final_database['Accession']  == user_mir]['ID']\n",
        "    filtered_database = final_database[final_database['Accession']  == user_mir]['ID']\n",
        "\n",
        "    if filtered_database.size == 1:\n",
        "        mir = filtered_database.iloc[0]\n",
        "        print('The accession number ' + user_mir + ' corresponds to miR ' + mir)\n",
        "    else:\n",
        "        print('miR accession is incorrect. Try again (caps sensitive)')\n",
        "\n",
        "    Entrez.email = 'anonymous@gmail.com'\n",
        "    esearch_query = Entrez.esearch(db=\"pubmed\", term=\"mir-100\", retmode=\"xml\")\n",
        "    esearch_result = Entrez.read(esearch_query)\n",
        "    pmid_list = esearch_result['IdList']\n",
        "    print(\"pmid's obtained: \" + str(len(pmid_list)))\n",
        "    \n",
        "    abs_list = []\n",
        "\n",
        "    for i in pmid_list:\n",
        "        abs = fetch_abstract(i)\n",
        "        abs_list.append(abs)\n",
        "        \n",
        "    abs_list = [concat_article(i) for i in abs_list if i is not None]\n",
        "    \n",
        "    return(abs_list)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ehsLbIHqXaA"
      },
      "source": [
        "### 2.4) Fetching Abstracts\r\n",
        "\r\n",
        "Now, we will maually enter the accession number of 10 miRNA to use to get literature on using our functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jDd_01Xa1Og"
      },
      "source": [
        "training_mir = ['MI0000692', 'MI0000159', 'MI0000172', 'MI0000406', 'MI0000111', 'MI0000684', 'MI0000256', 'MI0000170', 'MI0000268', 'MI0002470']"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIIMFzi2qxAG"
      },
      "source": [
        "Getting literature:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eLZbnXLa1Oh",
        "outputId": "0dd979ac-ebe0-4b31-c2d2-71f8de3b65e2"
      },
      "source": [
        "all_abstracts = []\n",
        "\n",
        "for i in training_mir:\n",
        "    abstracts = get_literature(i)\n",
        "    all_abstracts = all_abstracts + abstracts"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accession number MI0000692 corresponds to miR mmu-mir-100\n",
            "pmid's obtained: 20\n",
            "The accession number MI0000159 corresponds to miR mmu-mir-133a-1\n",
            "pmid's obtained: 20\n",
            "The accession number MI0000172 corresponds to miR mmu-mir-150\n",
            "pmid's obtained: 20\n",
            "The accession number MI0000406 corresponds to miR mmu-mir-106a\n",
            "pmid's obtained: 20\n",
            "The accession number MI0000111 corresponds to miR hsa-mir-105-1\n",
            "pmid's obtained: 20\n",
            "The accession number MI0000684 corresponds to miR mmu-mir-107\n",
            "pmid's obtained: 20\n",
            "The accession number MI0000256 corresponds to miR mmu-mir-122\n",
            "pmid's obtained: 20\n",
            "The accession number MI0000170 corresponds to miR mmu-mir-146a\n",
            "pmid's obtained: 20\n",
            "The accession number MI0000268 corresponds to miR hsa-mir-34a\n",
            "pmid's obtained: 20\n",
            "The accession number MI0002470 corresponds to miR hsa-mir-486-1\n",
            "pmid's obtained: 20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEwSu9huq0R4"
      },
      "source": [
        "# 3) **Text Processing**\r\n",
        "\r\n",
        "### 3.1) Sentence Tokenization\r\n",
        "\r\n",
        "We will split all the abstracts into sentence tokens, seeing as this is standard practice when creating a spacy NER."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDBi8V51a1Oh",
        "outputId": "7c27dbf3-a1de-4e49-a8b6-ada0bebbf660",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "all_sentences = []\n",
        "\n",
        "for mir_abs in all_abstracts:\n",
        "    abstr_sentences = sent_tokenize(mir_abs)\n",
        "    all_sentences = all_sentences + abstr_sentences\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTTI-scorNhh"
      },
      "source": [
        "### 3.2) Making training and testing data\r\n",
        "\r\n",
        "From our sentences, we will first randomly shuffle them:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Q6Ojt3mha2y"
      },
      "source": [
        "random.shuffle(all_sentences)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5tw8sE2rXnX"
      },
      "source": [
        "And then we will allocate 80% for training, and 20% for testing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rjy-kYGCha2z"
      },
      "source": [
        "training_sentences = all_sentences[0:int(.8 * len(all_sentences))]\n",
        "testing_sentences = all_sentences[int(.8 * len(all_sentences)):len(all_sentences)]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Is51lwgFrhln"
      },
      "source": [
        "### 3.3) Labelling the training data\r\n",
        "\r\n",
        "Now, we will use a regex to find miRNA occurances in a sentence. For each occurance we find, we will also locate the string index that corresponds to the start and end of the string, which is required by spacy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTAu95Mza1Ox"
      },
      "source": [
        "def make_training_data(string):\n",
        "    if len([i for i in re.finditer('mir-\\d+[^\\s|.|,|!|?| |:|;]*', string.lower())]) != 0:\n",
        "        ent_list = []\n",
        "        for i in re.finditer('mir-\\d+[^\\s|.|,|!|?| |:|;]*', string.lower()):\n",
        "            ent_code = (i.start(), i.end(), 'miR')\n",
        "            ent_list.append(ent_code)\n",
        "            \n",
        "    else:\n",
        "        ent_list = []      \n",
        "    return((string, {'entities' : ent_list}))    "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mu7AR86Ka1O0"
      },
      "source": [
        "training_data = [make_training_data(i) for i in training_sentences]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGYksK9QuLEs"
      },
      "source": [
        "An example of the format of the input data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkWlystea1O0",
        "outputId": "290eb340-f053-4d96-da84-4364ce768911"
      },
      "source": [
        "training_data[0:3]"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Using TarBase V8 in DIANA tools, we acquired 1,520 potential targets (mRNA) from the five key DE-miRNAs, among which the159 DE-mRNAs also included 11 DEGs.',\n",
              "  {'entities': []}),\n",
              " ('In conclusion, miR-100, miR-125b, miR-199a and miR-194 may have potential as prognostic and diagnostic biomarkers for GC.',\n",
              "  {'entities': [(15, 22, 'miR'),\n",
              "    (24, 32, 'miR'),\n",
              "    (34, 42, 'miR'),\n",
              "    (47, 54, 'miR')]}),\n",
              " ('This study aimed to identify differentially expressed (DE) miRNAs in breast cancer using the Cancer Genome Atlas.',\n",
              "  {'entities': []})]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TOa5H-Nu_1r"
      },
      "source": [
        "### 3.4) Data Exploration\r\n",
        "\r\n",
        "Pick up here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtQNfOkxa1O2",
        "outputId": "2ce88d9c-f2bb-43f3-c648-34bfe2286e1d"
      },
      "source": [
        "!pip install spacy-lookups-data"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy-lookups-data in /usr/local/lib/python3.7/dist-packages (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy-lookups-data) (54.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGO24gY0a1O2"
      },
      "source": [
        "def train_spacy(data,iterations):\n",
        "    TRAIN_DATA = data\n",
        "    nlp = spacy.blank('en')  # create blank Language class\n",
        "    # create the built-in pipeline components and add them to the pipeline\n",
        "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
        "    if 'ner' not in nlp.pipe_names:\n",
        "        ner = nlp.create_pipe('ner')\n",
        "        nlp.add_pipe(ner, last=True)\n",
        "       \n",
        "\n",
        "    # add labels\n",
        "    for _, annotations in TRAIN_DATA:\n",
        "         for ent in annotations.get('entities'):\n",
        "            ner.add_label(ent[2])\n",
        "\n",
        "    # get names of other pipes to disable them during training\n",
        "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
        "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
        "        optimizer = nlp.begin_training()\n",
        "        for itn in range(iterations):\n",
        "            print(\"Starting iteration \" + str(itn))\n",
        "            random.shuffle(TRAIN_DATA)\n",
        "            losses = {}\n",
        "            for text, annotations in TRAIN_DATA:\n",
        "                nlp.update(\n",
        "                    [text],  # batch of texts\n",
        "                    [annotations],  # batch of annotations\n",
        "                    drop=0.2,  # dropout - make it harder to memorise data\n",
        "                    sgd=optimizer,  # callable to update weights\n",
        "                    losses=losses)\n",
        "            print(losses)\n",
        "    return nlp"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MLex50Pa1O2",
        "outputId": "3b62b3a2-a89f-4c61-b62d-81510f64d707"
      },
      "source": [
        "miRnlp = train_spacy(training_data, 10)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting iteration 0\n",
            "{'ner': 724.416278879913}\n",
            "Starting iteration 1\n",
            "{'ner': 95.70888923383187}\n",
            "Starting iteration 2\n",
            "{'ner': 107.7119142875932}\n",
            "Starting iteration 3\n",
            "{'ner': 37.14966305430066}\n",
            "Starting iteration 4\n",
            "{'ner': 39.662877787169094}\n",
            "Starting iteration 5\n",
            "{'ner': 138.27446647558605}\n",
            "Starting iteration 6\n",
            "{'ner': 52.25174629664164}\n",
            "Starting iteration 7\n",
            "{'ner': 118.52414245554984}\n",
            "Starting iteration 8\n",
            "{'ner': 13.692096061201951}\n",
            "Starting iteration 9\n",
            "{'ner': 31.45547221134935}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIMgjHNGa1O4",
        "outputId": "b9cf3231-5876-431b-e76f-2873aa509a7f"
      },
      "source": [
        "for i in testing_sentences[0:5]:\n",
        "    print(i)\n",
        "    doc = miRnlp(i)\n",
        "    print(\"Entities\", [(ent.start_char, ent.end_char, ent.text, ent.label_) for ent in doc.ents])\n",
        "    print()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Here, a biosensor CPs/AuNP-AuE, the gold nanoparticle (AuNP)-modified Au electrode (AuE) which was coupled with DNA capture probes (CPs), was developed to detect the content of miR-100 in the sera of GC patients.\n",
            "Entities [(177, 184, 'miR-100', 'miR')]\n",
            "\n",
            "The aim of the current study was to evaluate whether hyperglycemia is able to affect the expression of selected miRNAs in VAT of prediabetic (IFG) and diabetic (T2DM) patients vs. normoglycemic (NG) subjects using qPCR.\n",
            "Entities []\n",
            "\n",
            "MiR-100, miR-125b and miR-199a predicted poor prognosis in GC, while miR-194 predicted favorable prognosis in GC.\n",
            "Entities [(0, 7, 'MiR-100', 'miR'), (9, 17, 'miR-125b', 'miR'), (22, 30, 'miR-199a', 'miR'), (69, 76, 'miR-194', 'miR')]\n",
            "\n",
            "Enrichment analyses indicated involvement of 11 top DE miRNAs in oxidative stress, inflammation and insulin signaling.\n",
            "Entities []\n",
            "\n",
            "For this purpose, MCF-7 and MDA-MB-435 cells were seeded different number in E-plate 16 for proliferation experiment using an electrical impedance-based real-time cell analyzer system (RTCA) for 168 h. Similarly, invasion potential of MCF-7 and MDA-MB-435 were determined by RTCA for 90 h. Total RNAs including miRNAs were isolated at 2, 4, 6, 12, 24, 48 h from the MCF-7 and MDA-MB-435 cells.\n",
            "Entities []\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdf29GMqo49W"
      },
      "source": [
        "def get_model_ents(sent):\r\n",
        "  doc = miRnlp(sent)\r\n",
        "  ent_list = []\r\n",
        "  for ent in doc.ents:\r\n",
        "    ent_code = str(ent.start_char) + '_' + str(ent.end_char) + '_' + str(ent.label_)\r\n",
        "    ent_list.append(ent_code)\r\n",
        "\r\n",
        "  return(ent_list)\r\n",
        "\r\n",
        "\r\n",
        "# {'entities': [(58, 68, 'miR')]})]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNCg2JNfq1vQ",
        "outputId": "9d3cd086-f4a4-402d-8a44-38cb1c67cff0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "[get_model_ents(i) for i in testing_sentences[0:10]]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['177_184_miR'],\n",
              " [],\n",
              " ['0_7_miR', '9_17_miR', '22_30_miR', '69_76_miR'],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " ['103_113_miR', '115_125_miR', '127_137_miR', '143_154_miR'],\n",
              " [],\n",
              " ['55_64_miR']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haXpB-tIrBkv"
      },
      "source": [
        "all_ent_predictions = [get_model_ents(i) for i in testing_sentences]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4saZDWurV3v"
      },
      "source": [
        "def check_testing_data(string):\r\n",
        "    if len([i for i in re.finditer('mir-\\d+[^\\s|.|,|!|?| |:|;]*', string.lower())]) != 0:\r\n",
        "        ent_list = []\r\n",
        "        for i in re.finditer('mir-\\d+[^\\s|.|,|!|?| |:|;]*', string.lower()):\r\n",
        "            ent_code = str(i.start()) + '_' + str(i.end()) + '_' + 'miR'\r\n",
        "            ent_list.append(ent_code)\r\n",
        "            \r\n",
        "    else:\r\n",
        "        ent_list = []      \r\n",
        "    return(ent_list)  "
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXKIawxKrd-m",
        "outputId": "e24ee416-49d2-403e-b9b0-7d9ae69ad22b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "[check_testing_data(i) for i in testing_sentences[0:10]]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['177_184_miR'],\n",
              " [],\n",
              " ['0_7_miR', '9_17_miR', '22_30_miR', '69_76_miR'],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " ['103_113_miR', '115_125_miR', '127_137_miR', '143_154_miR'],\n",
              " [],\n",
              " ['55_64_miR']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwkjTEaqsi4k"
      },
      "source": [
        "all_correct_labs = [check_testing_data(i) for i in testing_sentences]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UfuXJ-vtJyT"
      },
      "source": [
        "true_positive = 0\r\n",
        "false_positive = 0\r\n",
        "false_negative = 0\r\n",
        "\r\n",
        "for entry in range(len(all_correct_labs)):\r\n",
        "\r\n",
        "  for pred_label in range(len(all_ent_predictions[entry])):\r\n",
        "    true_positive += all_ent_predictions[entry][pred_label] in all_correct_labs[entry]\r\n",
        "    false_positive += all_ent_predictions[entry][pred_label] not in all_correct_labs[entry]\r\n",
        "  \r\n",
        "  for true_label in range(len(all_correct_labs[entry])):\r\n",
        "    false_negative += all_correct_labs[entry][pred_label] not in all_ent_predictions[entry]"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXrWILKvhLRe"
      },
      "source": [
        "![](https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/350px-Precisionrecall.svg.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bH6Fv8Nq0mDL",
        "outputId": "7541a93e-498d-44a0-b8c7-3fe00d73a367",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "miR_precision = true_positive / (true_positive + false_positive)\r\n",
        "miR_recall = true_positive / (true_positive + false_negative)\r\n",
        "\r\n",
        "print(\"Precision: \" + str(miR_precision))\r\n",
        "print(\"Recall: \" + str(miR_recall))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision: 1.0\n",
            "Recall: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqiNm0DLjdHj"
      },
      "source": [
        "![](https://miro.medium.com/max/1530/1*wUdjcIb9J9Bq6f2GvX1jSA.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYKT77AJjgtc",
        "outputId": "f8617788-a446-46b0-d074-ecf99365d2ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "miR_f1 = 2 * ((miR_precision * miR_recall) / (miR_precision + miR_recall))\r\n",
        "\r\n",
        "print(\"F1 Score: \" + str(miR_f1))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 Score: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}